{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maojinfang/AI-Agent-Journey-2026/blob/main/%E2%80%9CYOLO26.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6MPjfT5NrKQ"
      },
      "source": [
        "<div align=\"center\">\n",
        "  <a href=\"https://ultralytics.com/yolo\" target=\"_blank\">\n",
        "    <img width=\"1024\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/yolov8/banner-yolov8.png\">\n",
        "  </a>\n",
        "\n",
        "  [‰∏≠Êñá](https://docs.ultralytics.com/zh/) | [ÌïúÍµ≠Ïñ¥](https://docs.ultralytics.com/ko/) | [Êó•Êú¨Ë™û](https://docs.ultralytics.com/ja/) | [–†—É—Å—Å–∫–∏–π](https://docs.ultralytics.com/ru/) | [Deutsch](https://docs.ultralytics.com/de/) | [Fran√ßais](https://docs.ultralytics.com/fr/) | [Espa√±ol](https://docs.ultralytics.com/es/) | [Portugu√™s](https://docs.ultralytics.com/pt/) | [T√ºrk√ße](https://docs.ultralytics.com/tr/) | [Ti·∫øng Vi·ªát](https://docs.ultralytics.com/vi/) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](https://docs.ultralytics.com/ar/)\n",
        "\n",
        "  <a href=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml\"><img src=\"https://github.com/ultralytics/ultralytics/actions/workflows/ci.yml/badge.svg\" alt=\"Ultralytics CI\"></a>\n",
        "  <a href=\"https://console.paperspace.com/github/ultralytics/ultralytics\"><img src=\"https://assets.paperspace.io/img/gradient-badge.svg\" alt=\"Run on Gradient\"/></a>\n",
        "  <a href=\"https://colab.research.google.com/github/ultralytics/ultralytics/blob/main/examples/tutorial.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"></a>\n",
        "  <a href=\"https://www.kaggle.com/models/ultralytics/yolo26\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" alt=\"Open In Kaggle\"></a>\n",
        "\n",
        "  <a href=\"https://ultralytics.com/discord\"><img alt=\"Discord\" src=\"https://img.shields.io/discord/1089800235347353640?logo=discord&logoColor=white&label=Discord&color=blue\"></a>\n",
        "  <a href=\"https://community.ultralytics.com\"><img alt=\"Ultralytics Forums\" src=\"https://img.shields.io/discourse/users?server=https%3A%2F%2Fcommunity.ultralytics.com&logo=discourse&label=Forums&color=blue\"></a>\n",
        "  <a href=\"https://reddit.com/r/ultralytics\"><img alt=\"Ultralytics Reddit\" src=\"https://img.shields.io/reddit/subreddit-subscribers/ultralytics?style=flat&logo=reddit&logoColor=white&label=Reddit&color=blue\"></a>\n",
        "</div>\n",
        "\n",
        "This **Ultralytics Colab Notebook** is the easiest way to get started with [YOLO models](https://www.ultralytics.com/yolo)‚Äîno installation needed. Built by [Ultralytics](https://www.ultralytics.com/), the creators of YOLO, this notebook walks you through running **state-of-the-art** models directly in your browser.\n",
        "\n",
        "Ultralytics models are constantly updated for performance and flexibility. They're **fast**, **accurate**, and **easy to use**, and they excel at [object detection](https://docs.ultralytics.com/tasks/detect/), [tracking](https://docs.ultralytics.com/modes/track/), [instance segmentation](https://docs.ultralytics.com/tasks/segment/), [image classification](https://docs.ultralytics.com/tasks/classify/), and [pose estimation](https://docs.ultralytics.com/tasks/pose/).\n",
        "\n",
        "Find detailed documentation in the [Ultralytics Docs](https://docs.ultralytics.com/). Get support via [GitHub Issues](https://github.com/ultralytics/ultralytics/issues/new/choose). Join discussions on [Discord](https://discord.com/invite/ultralytics), [Reddit](https://www.reddit.com/r/ultralytics/), and the [Ultralytics Community Forums](https://community.ultralytics.com/)!\n",
        "\n",
        "Request an Enterprise License for commercial use at [Ultralytics Licensing](https://www.ultralytics.com/license)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7mGmQbAO5pQb"
      },
      "source": [
        "# Setup\n",
        "\n",
        "pip install `ultralytics` and [dependencies](https://github.com/ultralytics/ultralytics/blob/main/pyproject.toml) and check software and hardware.\n",
        "\n",
        "[![PyPI - Version](https://img.shields.io/pypi/v/ultralytics?logo=pypi&logoColor=white)](https://pypi.org/project/ultralytics/) [![Downloads](https://static.pepy.tech/badge/ultralytics)](https://clickpy.clickhouse.com/dashboard/ultralytics) [![PyPI - Python Version](https://img.shields.io/pypi/pyversions/ultralytics?logo=python&logoColor=gold)](https://pypi.org/project/ultralytics/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbvMlHd_QwMG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e769fae-8a01-4846-d027-bf8bd4bc890d"
      },
      "source": [
        "!uv pip install ultralytics\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.4.8 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 38.7/112.6 GB disk)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JnkELT0cIJg"
      },
      "source": [
        "# 1. Predict\n",
        "\n",
        "YOLO26 may be used directly in the Command Line Interface (CLI) with a `yolo` command for a variety of tasks and modes and accepts additional arguments, i.e. `imgsz=640`. See a full list of available `yolo` [arguments](https://docs.ultralytics.com/usage/cfg/) and other details in the [YOLO26 Predict Docs](https://docs.ultralytics.com/modes/predict/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zR9ZbuQCH7FX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0b164c2-1d90-42dd-8862-c67d8aae6886"
      },
      "source": [
        "# Run inference on an image with YOLO26n\n",
        "!yolo predict model=yolo26n.pt source='https://ultralytics.com/images/zidane.jpg'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'yolo26n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.3MB 75.2MB/s 0.1s\n",
            "Ultralytics 8.4.8 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO26n summary (fused): 122 layers, 2,408,932 parameters, 0 gradients, 5.4 GFLOPs\n",
            "\n",
            "\u001b[KDownloading https://ultralytics.com/images/zidane.jpg to 'zidane.jpg': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 49.2KB 51.3MB/s 0.0s\n",
            "image 1/1 /content/zidane.jpg: 384x640 2 persons, 1 tie, 55.4ms\n",
            "Speed: 12.2ms preprocess, 55.4ms inference, 38.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Results saved to \u001b[1m/content/runs/detect/predict\u001b[0m\n",
            "üí° Learn more at https://docs.ultralytics.com/modes/predict\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkAzDWJ7cWTr"
      },
      "source": [
        "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\n",
        "<img align=\"left\" src=\"https://user-images.githubusercontent.com/26833433/212889447-69e5bdf1-5800-4e29-835e-2ed2336dede2.jpg\" width=\"600\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eq1SMWl6Sfn"
      },
      "source": [
        "# 2. Val\n",
        "Validate a model's accuracy on the [COCO](https://docs.ultralytics.com/datasets/detect/coco/) dataset's `val` or `test` splits. The latest YOLO26 [models](https://github.com/ultralytics/ultralytics#models) are downloaded automatically the first time they are used. See [YOLO26 Val Docs](https://docs.ultralytics.com/modes/val/) for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X58w8JLpMnjH"
      },
      "source": [
        "# Download COCO val\n",
        "from ultralytics.utils.downloads import download\n",
        "\n",
        "download('https://ultralytics.com/assets/coco2017val.zip', unzip=True, dir='datasets') # download (780MB - 5000 images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZY2VXXXu74w5"
      },
      "source": [
        "# Validate YOLO26n on COCO8 val\n",
        "!yolo val model=yolo26n.pt data=coco8.yaml"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Train\n",
        "\n",
        "<p align=\"\"><a href=\"https://ultralytics.com/hub\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\"/></a></p>\n",
        "\n",
        "Train YOLO26 on [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/) datasets. See [YOLO26 Train Docs](https://docs.ultralytics.com/modes/train/) for more information."
      ],
      "metadata": {
        "id": "ktegpM42AooT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NcFxRcFdJ_O"
      },
      "source": [
        "#@title Select YOLO26 üöÄ logger {run: 'auto'}\n",
        "logger = 'TensorBoard' #@param ['TensorBoard', 'Weights & Biases']\n",
        "\n",
        "if logger == 'TensorBoard':\n",
        "  !yolo settings tensorboard=True\n",
        "  %load_ext tensorboard\n",
        "  %tensorboard --logdir .\n",
        "elif logger == 'Weights & Biases':\n",
        "  !yolo settings wandb=True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train YOLO26n on COCO8 for 3 epochs\n",
        "!yolo train model=yolo26n.pt data=coco8.yaml epochs=3 imgsz=640"
      ],
      "metadata": {
        "id": "nPZZeNrLCQG6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Export\n",
        "\n",
        "Export a YOLO model to any supported format below with the `format` argument, i.e. `format=onnx`. See [Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n",
        "\n",
        "- üí° ProTip: Export to [ONNX](https://docs.ultralytics.com/integrations/onnx/) or [OpenVINO](https://docs.ultralytics.com/integrations/openvino/) for up to 3x CPU speedup.\n",
        "- üí° ProTip: Export to [TensorRT](https://docs.ultralytics.com/integrations/tensorrt/) for up to 5x GPU speedup.\n",
        "\n",
        "| Format | `format` Argument | Model | Metadata | Arguments |\n",
        "|--------|-----------------|-------|----------|------------|\n",
        "| [PyTorch](https://pytorch.org/) | - | `yolo26n.pt` | ‚úÖ | - |\n",
        "| [TorchScript](https://docs.ultralytics.com/integrations/torchscript) | `torchscript` | `yolo26n.torchscript` | ‚úÖ | `imgsz`, `batch`, `dynamic`, `optimize`, `half`, `nms`, `device` |\n",
        "| [ONNX](https://docs.ultralytics.com/integrations/onnx) | `onnx` | `yolo26n.onnx` | ‚úÖ | `imgsz`, `batch`, `dynamic`, `half`, `opset`, `simplify`, `nms`, `device` |\n",
        "| [OpenVINO](https://docs.ultralytics.com/integrations/openvino) | `openvino` | `yolo26n_openvino_model/` | ‚úÖ | `imgsz`, `batch`, `dynamic`, `half`, `int8`, `nms`, `fraction`, `device`, `data` |\n",
        "| [TensorRT](https://docs.ultralytics.com/integrations/tensorrt) | `engine` | `yolo26n.engine` | ‚úÖ | `imgsz`, `batch`, `dynamic`, `half`, `int8`, `simplify`, `nms`, `fraction`, `device`, `data`, `workspace` |\n",
        "| [CoreML](https://docs.ultralytics.com/integrations/coreml) | `coreml` | `yolo26n.mlpackage` | ‚úÖ | `imgsz`, `batch`, `half`, `int8`, `nms`, `device` |\n",
        "| [TF SavedModel](https://docs.ultralytics.com/integrations/tf-savedmodel) | `saved_model` | `yolo26n_saved_model/` | ‚úÖ | `imgsz`, `batch`, `int8`, `keras`, `nms`, `device` |\n",
        "| [TF GraphDef](https://docs.ultralytics.com/integrations/tf-graphdef) | `pb` | `yolo26n.pb` | ‚ùå | `imgsz`, `batch`, `device` |\n",
        "| [TF Lite](https://docs.ultralytics.com/integrations/tflite) | `tflite` | `yolo26n.tflite` | ‚úÖ | `imgsz`, `batch`, `half`, `int8`, `nms`, `fraction`, `device`, `data` |\n",
        "| [TF Edge TPU](https://docs.ultralytics.com/integrations/edge-tpu) | `edgetpu` | `yolo26n_edgetpu.tflite` | ‚úÖ | `imgsz`, `device` |\n",
        "| [TF.js](https://docs.ultralytics.com/integrations/tfjs) | `tfjs` | `yolo26n_web_model/` | ‚úÖ | `imgsz`, `batch`, `half`, `int8`, `nms`, `device` |\n",
        "| [PaddlePaddle](https://docs.ultralytics.com/integrations/paddlepaddle) | `paddle` | `yolo26n_paddle_model/` | ‚úÖ | `imgsz`, `batch`, `device` |\n",
        "| [MNN](https://docs.ultralytics.com/integrations/mnn) | `mnn` | `yolo26n.mnn` | ‚úÖ | `imgsz`, `batch`, `half`, `int8`, `device` |\n",
        "| [NCNN](https://docs.ultralytics.com/integrations/ncnn) | `ncnn` | `yolo26n_ncnn_model/` | ‚úÖ | `imgsz`, `batch`, `half`, `device` |\n",
        "| [IMX500](https://docs.ultralytics.com/integrations/sony-imx500) | `imx` | `yolo26n_imx_model/` | ‚úÖ | `imgsz`, `int8`, `fraction`, `device`, `data` |\n",
        "| [RKNN](https://docs.ultralytics.com/integrations/rockchip-rknn) | `rknn` | `yolo26n_rknn_model/` | ‚úÖ | `imgsz`, `batch`, `name`, `device` |\n",
        "| [ExecuTorch](https://docs.ultralytics.com/integrations/executorch) | `executorch` | `yolo26n_executorch_model/` | ‚úÖ | `imgsz`, `device` |\n",
        "| [Axelera](https://docs.ultralytics.com/integrations/axelera) | `axelera` | `yolo26n_axelera_model/` | ‚úÖ | `imgsz`, `int8`, `fraction`, `device`, `data` |"
      ],
      "metadata": {
        "id": "CYIjW4igCjqD",
        "outputId": "9dd840c0-c689-49c6-f55a-b4cf366604fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!yolo export model=yolo26n.pt format=torchscript"
      ],
      "metadata": {
        "id": "kUMOQ0OeDBJG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Python Usage\n",
        "\n",
        "YOLO26 was reimagined using Python-first principles for the most seamless Python YOLO experience yet. YOLO26 models can be loaded from a trained checkpoint or created from scratch. Then methods are used to train, val, predict, and export the model. See detailed Python usage examples in the [YOLO26 Python Docs](https://docs.ultralytics.com/usage/python/)."
      ],
      "metadata": {
        "id": "bpF9-vS_DAaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a model\n",
        "model = YOLO('yolo26n.yaml')  # build a new model from scratch\n",
        "model = YOLO('yolo26n.pt')  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Use the model\n",
        "results = model.train(data='coco8.yaml', epochs=3)  # train the model\n",
        "results = model.val()  # evaluate model performance on the validation set\n",
        "results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
        "results = model.export(format='onnx')  # export the model to ONNX format"
      ],
      "metadata": {
        "id": "Phm9ccmOKye5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Tasks\n",
        "\n",
        "YOLO26 can train, val, predict and export models for the most common tasks in vision AI: [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/). See [YOLO26 Tasks Docs](https://docs.ultralytics.com/tasks/) for more information.\n",
        "\n",
        "<br><img width=\"1024\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png\">"
      ],
      "metadata": {
        "id": "yq26lwpYK1lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Detection\n",
        "\n",
        "YOLO26 _detection_ models have no suffix and are the default YOLO26 models, i.e. `yolo26n.pt` and are pretrained on COCO. See [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for full details."
      ],
      "metadata": {
        "id": "8Go5qqS9LbC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO26n, train it on COCO128 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo26n.pt')  # load a pretrained YOLO detection model\n",
        "model.train(data='coco8.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "7ZW58jUzK66B"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Segmentation\n",
        "\n",
        "YOLO26 _segmentation_ models use the `-seg` suffix, i.e. `yolo26n-seg.pt` and are pretrained on COCO. See [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for full details."
      ],
      "metadata": {
        "id": "WFPJIQl_L5HT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO26n-seg, train it on COCO128-seg for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo26n-seg.pt')  # load a pretrained YOLO segmentation model\n",
        "model.train(data='coco8-seg.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "ax3p94VNK9zR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Classification\n",
        "\n",
        "YOLO26 _classification_ models use the `-cls` suffix, i.e. `yolo26n-cls.pt` and are pretrained on ImageNet. See [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for full details."
      ],
      "metadata": {
        "id": "5q9Zu6zlL5rS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO26n-cls, train it on mnist160 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo26n-cls.pt')  # load a pretrained YOLO classification model\n",
        "model.train(data='mnist160', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "SpIaFLiO11TG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Pose\n",
        "\n",
        "YOLO26 _pose_ models use the `-pose` suffix, i.e. `yolo26n-pose.pt` and are pretrained on COCO Keypoints. See [Pose Docs](https://docs.ultralytics.com/tasks/pose/) for full details."
      ],
      "metadata": {
        "id": "si4aKFNg19vX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load YOLO26n-pose, train it on COCO8-pose for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo26n-pose.pt')  # load a pretrained YOLO pose model\n",
        "model.train(data='coco8-pose.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
      ],
      "metadata": {
        "id": "cf5j_T9-B5F0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Oriented Bounding Boxes (OBB)\n",
        "\n",
        "YOLO26 _OBB_ models use the `-obb` suffix, i.e. `yolo26n-obb.pt` and are pretrained on the DOTA dataset. See [OBB Docs](https://docs.ultralytics.com/tasks/obb/) for full details."
      ],
      "metadata": {
        "id": "IJNKClOOB5YS"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEijrePND_2I"
      },
      "source": [
        "# Load YOLO26n-obb, train it on DOTA8 for 3 epochs and predict an image with it\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolo26n-obb.pt')  # load a pretrained YOLO OBB model\n",
        "model.train(data='dota8.yaml', epochs=3)  # train the model\n",
        "model('https://ultralytics.com/images/boats.jpg')  # predict on an image"
      ],
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Appendix\n",
        "\n",
        "Additional content below."
      ],
      "metadata": {
        "id": "pIdE6i8C3LYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pip install from source\n",
        "!uv pip install git+https://github.com/ultralytics/ultralytics@main"
      ],
      "metadata": {
        "id": "uRKlwxSJdhd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Git clone and run tests on 'main' branch\n",
        "!git clone https://github.com/ultralytics/ultralytics -b main\n",
        "!uv pip install -qe ultralytics"
      ],
      "metadata": {
        "id": "GtPlh7mcCGZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run tests (Git clone only)\n",
        "!pytest ultralytics/tests"
      ],
      "metadata": {
        "id": "Wdc6t_bfzDDk",
        "outputId": "f50a7842-13b4-475e-fe65-584223a3c096",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n",
            "rootdir: /content/ultralytics\n",
            "configfile: pyproject.toml\n",
            "plugins: langsmith-0.6.4, typeguard-4.4.4, anyio-4.12.1\n",
            "collected 720 items                                                            \u001b[0m\n",
            "\n",
            "ultralytics/tests/test_cli.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m        [ 20%]\u001b[0m\n",
            "ultralytics/tests/test_cuda.py \u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                                     [ 23%]\u001b[0m\n",
            "ultralytics/tests/test_engine.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                  [ 27%]\u001b[0m\n",
            "ultralytics/tests/test_exports.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[31mF\u001b[0m\u001b[31m                           [ 34%]\u001b[0m\n",
            "ultralytics/tests/test_integrations.py \u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[31m                             [ 37%]\u001b[0m\n",
            "ultralytics/tests/test_python.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m [ 60%]\n",
            "\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m                                        [ 79%]\u001b[0m\n",
            "ultralytics/tests/test_solutions.py \u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[33ms\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[32m.\u001b[0m\u001b[31m  [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m_________________________________ test_checks __________________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_checks\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Validate CUDA settings against torch CUDA functions.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       \u001b[94massert\u001b[39;49;00m torch.cuda.is_available() == CUDA_IS_AVAILABLE\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: assert False == True\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +  where False = <function is_available at 0x77fd2c9c8220>()\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +    where <function is_available at 0x77fd2c9c8220> = <module 'torch.cuda' from '/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py'>.is_available\u001b[0m\n",
            "\u001b[1m\u001b[31mE        +      where <module 'torch.cuda' from '/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py'> = torch.cuda\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31multralytics/tests/test_cuda.py\u001b[0m:37: AssertionError\n",
            "\u001b[31m\u001b[1m___________________________________ test_amp ___________________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m DEVICES, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mNo CUDA devices available\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_amp\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Test AMP training checks.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       model = YOLO(\u001b[33m\"\u001b[39;49;00m\u001b[33myolo26n.pt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m).model.to(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mDEVICES[\u001b[94m0\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31multralytics/tests/test_cuda.py\u001b[0m:44: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:1371: in to\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._apply(convert)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/nn/tasks.py\u001b[0m:286: in _apply\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m = \u001b[96msuper\u001b[39;49;00m()._apply(fn)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:930: in _apply\n",
            "    \u001b[0mmodule._apply(fn)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:930: in _apply\n",
            "    \u001b[0mmodule._apply(fn)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:930: in _apply\n",
            "    \u001b[0mmodule._apply(fn)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:957: in _apply\n",
            "    \u001b[0mparam_applied = fn(param)\u001b[90m\u001b[39;49;00m\n",
            "                    ^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:1357: in convert\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m t.to(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_lazy_init\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mglobal\u001b[39;49;00m _initialized, _queued_calls\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m is_initialized() \u001b[95mor\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(_tls, \u001b[33m\"\u001b[39;49;00m\u001b[33mis_initializing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mreturn\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mwith\u001b[39;49;00m _initialization_lock:\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# We be double-checked locking, boys!  This is OK because\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# the above test was GIL protected anyway.  The inner test\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# is for when a thread blocked on some other thread which was\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# doing the initialization; when they get the lock, they will\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# find there is nothing left to do.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m is_initialized():\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mreturn\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# It is important to prevent other threads from entering _lazy_init\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# immediately, while we are still guaranteed to have the GIL, because some\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# of the C calls we make below will release the GIL\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m _is_in_bad_fork():\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mraise\u001b[39;49;00m \u001b[96mRuntimeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mmultiprocessing, you must use the \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mspawn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m start method\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                )\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(torch._C, \u001b[33m\"\u001b[39;49;00m\u001b[33m_cuda_getDeviceCount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mraise\u001b[39;49;00m \u001b[96mAssertionError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTorch not compiled with CUDA enabled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m _cudart \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mraise\u001b[39;49;00m \u001b[96mAssertionError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                )\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# This function throws if there's a driver initialization error, no GPUs\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# are found or any other error occurs\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">           torch._C._cuda_init()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: No CUDA GPUs are available\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m:410: RuntimeError\n",
            "\u001b[31m\u001b[1m__________________________________ test_train __________________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m DEVICES, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mNo CUDA devices available\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_train\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Test model training on a minimal dataset using available CUDA devices.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[96mtuple\u001b[39;49;00m(DEVICES) \u001b[94mif\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(DEVICES) > \u001b[94m1\u001b[39;49;00m \u001b[94melse\u001b[39;49;00m DEVICES[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# NVIDIA Jetson only has one GPU and therefore skipping checks\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m IS_JETSON:\u001b[90m\u001b[39;49;00m\n",
            ">           results = YOLO(MODEL).train(data=\u001b[33m\"\u001b[39;49;00m\u001b[33mcoco8.yaml\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, imgsz=\u001b[94m64\u001b[39;49;00m, epochs=\u001b[94m1\u001b[39;49;00m, device=device, batch=\u001b[94m15\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31multralytics/tests/test_cuda.py\u001b[0m:123: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31multralytics/ultralytics/engine/model.py\u001b[0m:769: in train\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.trainer = (trainer \u001b[95mor\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._smart_load(\u001b[33m\"\u001b[39;49;00m\u001b[33mtrainer\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))(overrides=args, _callbacks=\u001b[96mself\u001b[39;49;00m.callbacks)\u001b[90m\u001b[39;49;00m\n",
            "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/models/yolo/detect/train.py\u001b[0m:63: in __init__\n",
            "    \u001b[0m\u001b[96msuper\u001b[39;49;00m().\u001b[92m__init__\u001b[39;49;00m(cfg, overrides, _callbacks)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/engine/trainer.py\u001b[0m:128: in __init__\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.device = select_device(\u001b[96mself\u001b[39;49;00m.args.device)\u001b[90m\u001b[39;49;00m\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "device = '0', newline = False, verbose = True\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mselect_device\u001b[39;49;00m(device=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, newline=\u001b[94mFalse\u001b[39;49;00m, verbose=\u001b[94mTrue\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Select the appropriate PyTorch device based on the provided arguments.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    The function takes a string specifying the device or a torch.device object and returns a torch.device object\u001b[39;49;00m\n",
            "    \u001b[33m    representing the selected device. The function also validates the number of available devices and raises an\u001b[39;49;00m\n",
            "    \u001b[33m    exception if the requested device(s) are not available.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        device (str | torch.device, optional): Device string or torch.device object. Options are 'None', 'cpu', or\u001b[39;49;00m\n",
            "    \u001b[33m            'cuda', or '0' or '0,1,2,3'. Auto-selects the first available GPU, or CPU if no GPU is available.\u001b[39;49;00m\n",
            "    \u001b[33m        newline (bool, optional): If True, adds a newline at the end of the log string.\u001b[39;49;00m\n",
            "    \u001b[33m        verbose (bool, optional): If True, logs the device information.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        (torch.device): Selected device.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Examples:\u001b[39;49;00m\n",
            "    \u001b[33m        >>> select_device(\"cuda:0\")\u001b[39;49;00m\n",
            "    \u001b[33m        device(type='cuda', index=0)\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m        >>> select_device(\"cpu\")\u001b[39;49;00m\n",
            "    \u001b[33m        device(type='cpu')\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Notes:\u001b[39;49;00m\n",
            "    \u001b[33m        Sets the 'CUDA_VISIBLE_DEVICES' environment variable for specifying which GPUs to use.\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(device, torch.device) \u001b[95mor\u001b[39;49;00m \u001b[96mstr\u001b[39;49;00m(device).startswith((\u001b[33m\"\u001b[39;49;00m\u001b[33mtpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mintel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mvulkan\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)):\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mreturn\u001b[39;49;00m device\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        s = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mUltralytics \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m__version__\u001b[33m}\u001b[39;49;00m\u001b[33m üöÄ Python-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mPYTHON_VERSION\u001b[33m}\u001b[39;49;00m\u001b[33m torch-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mTORCH_VERSION\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[96mstr\u001b[39;49;00m(device).lower()\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mfor\u001b[39;49;00m remove \u001b[95min\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m(\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m[\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            device = device.replace(remove, \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)  \u001b[90m# to string, 'cuda:0' -> '0' and '(0, 1)' -> '0,1'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Auto-select GPUs\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m device:\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96multralytics\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mutils\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mautodevice\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m GPUInfo\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# Replace each -1 with a selected GPU or remove it\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            parts = device.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            selected = GPUInfo().select_idle_gpu(count=parts.count(\u001b[33m\"\u001b[39;49;00m\u001b[33m-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), min_memory_fraction=\u001b[94m0.2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(parts)):\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mif\u001b[39;49;00m parts[i] == \u001b[33m\"\u001b[39;49;00m\u001b[33m-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "                    parts[i] = \u001b[96mstr\u001b[39;49;00m(selected.pop(\u001b[94m0\u001b[39;49;00m)) \u001b[94mif\u001b[39;49;00m selected \u001b[94melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            device = \u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(p \u001b[94mfor\u001b[39;49;00m p \u001b[95min\u001b[39;49;00m parts \u001b[94mif\u001b[39;49;00m p)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        cpu = device == \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        mps = device \u001b[95min\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mmps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mmps:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}  \u001b[90m# Apple Metal Performance Shaders (MPS)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m cpu \u001b[95mor\u001b[39;49;00m mps:\u001b[90m\u001b[39;49;00m\n",
            "            os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mCUDA_VISIBLE_DEVICES\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m  \u001b[90m# force torch.cuda.is_available() = False\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melif\u001b[39;49;00m device:  \u001b[90m# non-cpu device requested\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m device == \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "                device = \u001b[33m\"\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m device:\u001b[90m\u001b[39;49;00m\n",
            "                device = \u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([x \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m device.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m x])  \u001b[90m# remove sequential commas, i.e. \"0,,1\" -> \"0,1\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            visible = os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mCUDA_VISIBLE_DEVICES\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mCUDA_VISIBLE_DEVICES\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = device  \u001b[90m# set environment variable - must be before assert is_available()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m (torch.cuda.is_available() \u001b[95mand\u001b[39;49;00m torch.cuda.device_count() >= \u001b[96mlen\u001b[39;49;00m(device.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))):\u001b[90m\u001b[39;49;00m\n",
            "                LOGGER.info(s)\u001b[90m\u001b[39;49;00m\n",
            "                install = (\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mCUDA devices are seen by torch.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[94mif\u001b[39;49;00m torch.cuda.device_count() == \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[94melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                )\u001b[90m\u001b[39;49;00m\n",
            ">               \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInvalid CUDA \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mdevice=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdevice\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m requested.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m Use \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mdevice=cpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m or pass valid CUDA device(s) if available,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m i.e. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mdevice=0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m or \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mdevice=0,1,2,3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m for Multi-GPU.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mtorch.cuda.is_available(): \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtorch.cuda.is_available()\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mtorch.cuda.device_count(): \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtorch.cuda.device_count()\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mos.environ[\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCUDA_VISIBLE_DEVICES\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m]: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvisible\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00minstall\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE               ValueError: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\u001b[0m\n",
            "\u001b[1m\u001b[31mE               \u001b[0m\n",
            "\u001b[1m\u001b[31mE               torch.cuda.is_available(): False\u001b[0m\n",
            "\u001b[1m\u001b[31mE               torch.cuda.device_count(): 1\u001b[0m\n",
            "\u001b[1m\u001b[31mE               os.environ['CUDA_VISIBLE_DEVICES']:\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/utils/torch_utils.py\u001b[0m:199: ValueError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolo26n.pt to 'weights/path with spaces/yolo26n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.3MB 101.5MB/s 0.1s\n",
            "Ultralytics 8.4.7 üöÄ Python-3.12.12 torch-2.9.0+cu126 \n",
            "\u001b[31m\u001b[1m________________________________ test_autobatch ________________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m DEVICES, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mNo CUDA devices available\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_autobatch\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Check optimal batch size for YOLO model training using autobatch utility.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96multralytics\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mutils\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mautobatch\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m check_train_batch_size\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            ">       check_train_batch_size(YOLO(MODEL).model.to(\u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mcuda:\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mDEVICES[\u001b[94m0\u001b[39;49;00m]\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), imgsz=\u001b[94m128\u001b[39;49;00m, amp=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31multralytics/tests/test_cuda.py\u001b[0m:168: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:1371: in to\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._apply(convert)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/nn/tasks.py\u001b[0m:286: in _apply\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m = \u001b[96msuper\u001b[39;49;00m()._apply(fn)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:930: in _apply\n",
            "    \u001b[0mmodule._apply(fn)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:930: in _apply\n",
            "    \u001b[0mmodule._apply(fn)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:930: in _apply\n",
            "    \u001b[0mmodule._apply(fn)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:957: in _apply\n",
            "    \u001b[0mparam_applied = fn(param)\u001b[90m\u001b[39;49;00m\n",
            "                    ^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m:1357: in convert\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m t.to(\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m_lazy_init\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mglobal\u001b[39;49;00m _initialized, _queued_calls\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m is_initialized() \u001b[95mor\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(_tls, \u001b[33m\"\u001b[39;49;00m\u001b[33mis_initializing\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mreturn\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mwith\u001b[39;49;00m _initialization_lock:\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# We be double-checked locking, boys!  This is OK because\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# the above test was GIL protected anyway.  The inner test\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# is for when a thread blocked on some other thread which was\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# doing the initialization; when they get the lock, they will\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# find there is nothing left to do.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m is_initialized():\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mreturn\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# It is important to prevent other threads from entering _lazy_init\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# immediately, while we are still guaranteed to have the GIL, because some\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# of the C calls we make below will release the GIL\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m _is_in_bad_fork():\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mraise\u001b[39;49;00m \u001b[96mRuntimeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mmultiprocessing, you must use the \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mspawn\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m start method\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                )\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m \u001b[96mhasattr\u001b[39;49;00m(torch._C, \u001b[33m\"\u001b[39;49;00m\u001b[33m_cuda_getDeviceCount\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mraise\u001b[39;49;00m \u001b[96mAssertionError\u001b[39;49;00m(\u001b[33m\"\u001b[39;49;00m\u001b[33mTorch not compiled with CUDA enabled\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m _cudart \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mraise\u001b[39;49;00m \u001b[96mAssertionError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                )\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# This function throws if there's a driver initialization error, no GPUs\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# are found or any other error occurs\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">           torch._C._cuda_init()\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE           RuntimeError: No CUDA GPUs are available\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/cuda/__init__.py\u001b[0m:410: RuntimeError\n",
            "\u001b[31m\u001b[1m_______________________________ test_predict_sam _______________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m DEVICES, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mNo CUDA devices available\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_predict_sam\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Test SAM model predictions using different prompts.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96multralytics\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m SAM\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96multralytics\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96msam\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m Predictor \u001b[94mas\u001b[39;49;00m SAMPredictor\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        model = SAM(WEIGHTS_DIR / \u001b[33m\"\u001b[39;49;00m\u001b[33msam2.1_b.pt\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        model.info()\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Run inference with various prompts\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       model(SOURCE, device=DEVICES[\u001b[94m0\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31multralytics/tests/test_cuda.py\u001b[0m:200: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31multralytics/ultralytics/models/sam/model.py\u001b[0m:134: in __call__\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m.predict(source, stream, bboxes, points, labels, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/models/sam/model.py\u001b[0m:109: in predict\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m \u001b[96msuper\u001b[39;49;00m().predict(source, stream, prompts=prompts, **kwargs)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/engine/model.py\u001b[0m:529: in predict\n",
            "    \u001b[0m\u001b[96mself\u001b[39;49;00m.predictor.setup_model(model=\u001b[96mself\u001b[39;49;00m.model, verbose=is_cli)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/models/sam/predict.py\u001b[0m:454: in setup_model\n",
            "    \u001b[0mdevice = select_device(\u001b[96mself\u001b[39;49;00m.args.device, verbose=verbose)\u001b[90m\u001b[39;49;00m\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "device = '0', newline = False, verbose = False\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mselect_device\u001b[39;49;00m(device=\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, newline=\u001b[94mFalse\u001b[39;49;00m, verbose=\u001b[94mTrue\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Select the appropriate PyTorch device based on the provided arguments.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    The function takes a string specifying the device or a torch.device object and returns a torch.device object\u001b[39;49;00m\n",
            "    \u001b[33m    representing the selected device. The function also validates the number of available devices and raises an\u001b[39;49;00m\n",
            "    \u001b[33m    exception if the requested device(s) are not available.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Args:\u001b[39;49;00m\n",
            "    \u001b[33m        device (str | torch.device, optional): Device string or torch.device object. Options are 'None', 'cpu', or\u001b[39;49;00m\n",
            "    \u001b[33m            'cuda', or '0' or '0,1,2,3'. Auto-selects the first available GPU, or CPU if no GPU is available.\u001b[39;49;00m\n",
            "    \u001b[33m        newline (bool, optional): If True, adds a newline at the end of the log string.\u001b[39;49;00m\n",
            "    \u001b[33m        verbose (bool, optional): If True, logs the device information.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Returns:\u001b[39;49;00m\n",
            "    \u001b[33m        (torch.device): Selected device.\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Examples:\u001b[39;49;00m\n",
            "    \u001b[33m        >>> select_device(\"cuda:0\")\u001b[39;49;00m\n",
            "    \u001b[33m        device(type='cuda', index=0)\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m        >>> select_device(\"cpu\")\u001b[39;49;00m\n",
            "    \u001b[33m        device(type='cpu')\u001b[39;49;00m\n",
            "    \u001b[33m\u001b[39;49;00m\n",
            "    \u001b[33m    Notes:\u001b[39;49;00m\n",
            "    \u001b[33m        Sets the 'CUDA_VISIBLE_DEVICES' environment variable for specifying which GPUs to use.\u001b[39;49;00m\n",
            "    \u001b[33m    \"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(device, torch.device) \u001b[95mor\u001b[39;49;00m \u001b[96mstr\u001b[39;49;00m(device).startswith((\u001b[33m\"\u001b[39;49;00m\u001b[33mtpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mintel\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mvulkan\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)):\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mreturn\u001b[39;49;00m device\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        s = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mUltralytics \u001b[39;49;00m\u001b[33m{\u001b[39;49;00m__version__\u001b[33m}\u001b[39;49;00m\u001b[33m üöÄ Python-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mPYTHON_VERSION\u001b[33m}\u001b[39;49;00m\u001b[33m torch-\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mTORCH_VERSION\u001b[33m}\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        device = \u001b[96mstr\u001b[39;49;00m(device).lower()\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mfor\u001b[39;49;00m remove \u001b[95min\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda:\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mnone\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m(\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m)\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m[\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m]\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            device = device.replace(remove, \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)  \u001b[90m# to string, 'cuda:0' -> '0' and '(0, 1)' -> '0,1'\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Auto-select GPUs\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m device:\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96multralytics\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mutils\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mautodevice\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m GPUInfo\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "            \u001b[90m# Replace each -1 with a selected GPU or remove it\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            parts = device.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            selected = GPUInfo().select_idle_gpu(count=parts.count(\u001b[33m\"\u001b[39;49;00m\u001b[33m-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), min_memory_fraction=\u001b[94m0.2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(parts)):\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[94mif\u001b[39;49;00m parts[i] == \u001b[33m\"\u001b[39;49;00m\u001b[33m-1\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "                    parts[i] = \u001b[96mstr\u001b[39;49;00m(selected.pop(\u001b[94m0\u001b[39;49;00m)) \u001b[94mif\u001b[39;49;00m selected \u001b[94melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            device = \u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join(p \u001b[94mfor\u001b[39;49;00m p \u001b[95min\u001b[39;49;00m parts \u001b[94mif\u001b[39;49;00m p)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        cpu = device == \u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        mps = device \u001b[95min\u001b[39;49;00m {\u001b[33m\"\u001b[39;49;00m\u001b[33mmps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mmps:0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m}  \u001b[90m# Apple Metal Performance Shaders (MPS)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m cpu \u001b[95mor\u001b[39;49;00m mps:\u001b[90m\u001b[39;49;00m\n",
            "            os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mCUDA_VISIBLE_DEVICES\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m  \u001b[90m# force torch.cuda.is_available() = False\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94melif\u001b[39;49;00m device:  \u001b[90m# non-cpu device requested\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m device == \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "                device = \u001b[33m\"\u001b[39;49;00m\u001b[33m0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[95min\u001b[39;49;00m device:\u001b[90m\u001b[39;49;00m\n",
            "                device = \u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m.join([x \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m device.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m) \u001b[94mif\u001b[39;49;00m x])  \u001b[90m# remove sequential commas, i.e. \"0,,1\" -> \"0,1\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            visible = os.environ.get(\u001b[33m\"\u001b[39;49;00m\u001b[33mCUDA_VISIBLE_DEVICES\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[94mNone\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "            os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mCUDA_VISIBLE_DEVICES\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m] = device  \u001b[90m# set environment variable - must be before assert is_available()\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m (torch.cuda.is_available() \u001b[95mand\u001b[39;49;00m torch.cuda.device_count() >= \u001b[96mlen\u001b[39;49;00m(device.split(\u001b[33m\"\u001b[39;49;00m\u001b[33m,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m))):\u001b[90m\u001b[39;49;00m\n",
            "                LOGGER.info(s)\u001b[90m\u001b[39;49;00m\n",
            "                install = (\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mSee https://pytorch.org/get-started/locally/ for up-to-date torch install instructions if no \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33m\"\u001b[39;49;00m\u001b[33mCUDA devices are seen by torch.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[94mif\u001b[39;49;00m torch.cuda.device_count() == \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[94melse\u001b[39;49;00m \u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                )\u001b[90m\u001b[39;49;00m\n",
            ">               \u001b[94mraise\u001b[39;49;00m \u001b[96mValueError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInvalid CUDA \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mdevice=\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mdevice\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m requested.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m Use \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mdevice=cpu\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m or pass valid CUDA device(s) if available,\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m i.e. \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mdevice=0\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m or \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mdevice=0,1,2,3\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m for Multi-GPU.\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mtorch.cuda.is_available(): \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtorch.cuda.is_available()\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mtorch.cuda.device_count(): \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mtorch.cuda.device_count()\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33mos.environ[\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33mCUDA_VISIBLE_DEVICES\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m]: \u001b[39;49;00m\u001b[33m{\u001b[39;49;00mvisible\u001b[33m}\u001b[39;49;00m\u001b[33m\\n\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00minstall\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE               ValueError: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass valid CUDA device(s) if available, i.e. 'device=0' or 'device=0,1,2,3' for Multi-GPU.\u001b[0m\n",
            "\u001b[1m\u001b[31mE               \u001b[0m\n",
            "\u001b[1m\u001b[31mE               torch.cuda.is_available(): False\u001b[0m\n",
            "\u001b[1m\u001b[31mE               torch.cuda.device_count(): 1\u001b[0m\n",
            "\u001b[1m\u001b[31mE               os.environ['CUDA_VISIBLE_DEVICES']: 0\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/utils/torch_utils.py\u001b[0m:199: ValueError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/sam2.1_b.pt to 'weights/sam2.1_b.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 154.4MB 114.7MB/s 1.3s\n",
            "Model summary: 436 layers, 80,850,178 parameters, 80,850,178 gradients\n",
            "Ultralytics 8.4.7 üöÄ Python-3.12.12 torch-2.9.0+cu126 \n",
            "\u001b[31m\u001b[1m____________________________ test_export_executorch ____________________________\u001b[0m\n",
            "\n",
            "    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.skipif(\u001b[95mnot\u001b[39;49;00m checks.IS_PYTHON_MINIMUM_3_10 \u001b[95mor\u001b[39;49;00m \u001b[95mnot\u001b[39;49;00m TORCH_2_9, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mRequires Python>=3.10 and Torch>=2.9.0\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[37m@pytest\u001b[39;49;00m.mark.skipif(WINDOWS, reason=\u001b[33m\"\u001b[39;49;00m\u001b[33mSkipping test on Windows\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_export_executorch\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m    \u001b[39;49;00m\u001b[33m\"\"\"Test YOLO model export to ExecuTorch format.\"\"\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            ">       file = YOLO(MODEL).export(\u001b[96mformat\u001b[39;49;00m=\u001b[33m\"\u001b[39;49;00m\u001b[33mexecutorch\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, imgsz=\u001b[94m32\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\n",
            "\u001b[1m\u001b[31multralytics/tests/test_exports.py\u001b[0m:309: \n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\u001b[1m\u001b[31multralytics/ultralytics/engine/model.py\u001b[0m:710: in export\n",
            "    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Exporter(overrides=args, _callbacks=\u001b[96mself\u001b[39;49;00m.callbacks)(model=\u001b[96mself\u001b[39;49;00m.model)\u001b[90m\u001b[39;49;00m\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/engine/exporter.py\u001b[0m:604: in __call__\n",
            "    \u001b[0mf[\u001b[94m15\u001b[39;49;00m] = \u001b[96mself\u001b[39;49;00m.export_executorch()\u001b[90m\u001b[39;49;00m\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/engine/exporter.py\u001b[0m:249: in outer_func\n",
            "    \u001b[0m\u001b[94mraise\u001b[39;49;00m e\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/engine/exporter.py\u001b[0m:241: in outer_func\n",
            "    \u001b[0mf = inner_func(*args, **kwargs)  \u001b[90m# exported file/dir or tuple of (file/dir, *)\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31multralytics/ultralytics/engine/exporter.py\u001b[0m:1210: in export_executorch\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mbackends\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mxnnpack\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mpartition\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mxnnpack_partitioner\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m XnnpackPartitioner\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/backends/xnnpack/__init__.py\u001b[0m:8: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mpartition\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mxnnpack_partitioner\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/backends/xnnpack/partition/xnnpack_partitioner.py\u001b[0m:12: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mbackends\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mxnnpack\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mpartition\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mconfig\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m ALL_PARTITIONER_CONFIGS\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/backends/xnnpack/partition/config/__init__.py\u001b[0m:10: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mbackends\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mxnnpack\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mpartition\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mconfig\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mgemm_configs\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/backends/xnnpack/partition/config/gemm_configs.py\u001b[0m:12: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mbackends\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mtransforms\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m get_shape\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/backends/transforms/__init__.py\u001b[0m:9: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mbackends\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mtransforms\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96maddmm_mm_to_linear\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m (  \u001b[90m# noqa: F401\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/backends/transforms/addmm_mm_to_linear.py\u001b[0m:8: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mexir\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mdialects\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96m_ops\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m ops \u001b[94mas\u001b[39;49;00m exir_ops\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/exir/__init__.py\u001b[0m:9: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mexir\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mcapture\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/exir/capture/__init__.py\u001b[0m:9: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mexir\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mcapture\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96m_capture\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/exir/capture/_capture.py\u001b[0m:15: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mexir\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mcapture\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96m_config\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m CaptureConfig\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/exir/capture/_config.py\u001b[0m:15: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mexir\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mpasses\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m MemoryPlanningPass, ToOutVarPass\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/exir/passes/__init__.py\u001b[0m:19: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mexir\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m control_flow, memory, memory_planning\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/exir/control_flow.py\u001b[0m:58: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mexir\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mtracer\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m (\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/exir/tracer.py\u001b[0m:44: in <module>\n",
            "    \u001b[0m\u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mexecutorch\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mexir\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96moperator\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mutil\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m _QUANT_PRIMITIVES\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/executorch/exir/operator/util.py\u001b[0m:62: in <module>\n",
            "    \u001b[0mtorch.ops.torchao.dequantize_affine.default,\u001b[90m\u001b[39;49;00m\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n",
            "\n",
            "self = <module 'torch.ops.torchao' from 'torch.ops'>\n",
            "op_name = 'dequantize_affine'\n",
            "\n",
            "    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92m__getattr__\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, op_name: \u001b[96mstr\u001b[39;49;00m) -> OpOverloadPacket:\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mif\u001b[39;49;00m op_name \u001b[95min\u001b[39;49;00m (\u001b[33m\"\u001b[39;49;00m\u001b[33m__origin__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33m__self__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "                \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33mInvalid attribute \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mop_name\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m for \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m_OpNamespace\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mself\u001b[39;49;00m.name\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "            )\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# Get the op `my_namespace::my_op` if available. This will also check\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# for overloads and raise an exception if there are more than one.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        namespace_name = \u001b[96mself\u001b[39;49;00m.name\u001b[90m\u001b[39;49;00m\n",
            "        qualified_op_name = \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mnamespace_name\u001b[33m}\u001b[39;49;00m\u001b[33m::\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mop_name\u001b[33m}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        module_name = \u001b[96mself\u001b[39;49;00m.\u001b[91m__module__\u001b[39;49;00m + \u001b[33m\"\u001b[39;49;00m\u001b[33m.\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m + namespace_name\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[94mtry\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            "            op, overload_names = _get_packet(qualified_op_name, module_name)\u001b[90m\u001b[39;49;00m\n",
            "            \u001b[94mif\u001b[39;49;00m op \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n",
            ">               \u001b[94mraise\u001b[39;49;00m \u001b[96mAttributeError\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n",
            "                    \u001b[33mf\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m_OpNamespace\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00m\u001b[96mself\u001b[39;49;00m.name\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m object has no attribute \u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00mop_name\u001b[33m}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "                )\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE               AttributeError: '_OpNamespace' 'torchao' object has no attribute 'dequantize_affine'\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31m/usr/local/lib/python3.12/dist-packages/torch/_ops.py\u001b[0m:1365: AttributeError\n",
            "----------------------------- Captured stdout call -----------------------------\n",
            "Ultralytics 8.4.7 üöÄ Python-3.12.12 torch-2.9.0+cu126 CPU (Intel Xeon CPU @ 2.00GHz)\n",
            "WARNING ‚ö†Ô∏è EXECUTORCH export does not support end2end models, disabling end2end branch.\n",
            "YOLO26n summary (fused): 146 layers, 2,562,496 parameters, 0 gradients, 5.4 GFLOPs\n",
            "\n",
            "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'weights/path with spaces/yolo26n.pt' with input shape (1, 3, 32, 32) BCHW and output shape(s) (1, 84, 21) (5.3 MB)\n",
            "\n",
            "\u001b[34m\u001b[1mExecuTorch:\u001b[0m starting export with ExecuTorch...\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['ruamel.yaml<0.19.0'] not found, attempting AutoUpdate...\n",
            "Using Python 3.12.12 environment at: /usr\n",
            "Resolved 2 packages in 187ms\n",
            "Prepared 2 packages in 46ms\n",
            "Installed 2 packages in 3ms\n",
            " + ruamel-yaml==0.18.17\n",
            " + ruamel-yaml-clib==0.2.15\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 0.9s\n",
            "WARNING ‚ö†Ô∏è \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m Ultralytics requirement ['executorch==1.0.1'] not found, attempting AutoUpdate...\n",
            "Using Python 3.12.12 environment at: /usr\n",
            "Resolved 79 packages in 505ms\n",
            "Prepared 14 packages in 852ms\n",
            "Uninstalled 2 packages in 111ms\n",
            "Installed 14 packages in 86ms\n",
            " + execnet==2.1.2\n",
            " + executorch==1.0.1\n",
            " + expecttest==0.3.0\n",
            " + hydra-core==1.3.2\n",
            " + hypothesis==6.151.2\n",
            " + kgb==7.3\n",
            " + parameterized==0.9.0\n",
            " + pytest-json-report==1.5.0\n",
            " + pytest-metadata==3.1.1\n",
            " + pytest-rerunfailures==15.1\n",
            " + pytest-xdist==3.8.0\n",
            " + pytorch-tokenizers==1.1.0\n",
            " - scikit-learn==1.6.1\n",
            " + scikit-learn==1.7.1\n",
            " - torchao==0.10.0\n",
            " + torchao==0.14.0\n",
            "\n",
            "\u001b[31m\u001b[1mrequirements:\u001b[0m AutoUpdate success ‚úÖ 1.7s\n",
            "WARNING ‚ö†Ô∏è \u001b[31m\u001b[1mrequirements:\u001b[0m \u001b[1mRestart runtime or rerun command for updates to take effect\u001b[0m\n",
            "\n",
            "ERROR ‚ùå \u001b[34m\u001b[1mExecuTorch:\u001b[0m export failure 2.8s: '_OpNamespace' 'torchao' object has no attribute 'dequantize_affine'\n",
            "\u001b[33m=============================== warnings summary ===============================\u001b[0m\n",
            "tests/test_engine.py: 1 warning\n",
            "tests/test_exports.py: 5 warnings\n",
            "tests/test_python.py: 7 warnings\n",
            "  /content/ultralytics/ultralytics/nn/modules/head.py:178: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "    if self.dynamic or self.shape != shape:\n",
            "\n",
            "tests/test_exports.py::test_export_onnx[False]\n",
            "tests/test_exports.py::test_export_onnx[True]\n",
            "tests/test_exports.py::test_export_tflite\n",
            "tests/test_python.py::test_utils_benchmarks\n",
            "tests/test_python.py::test_multichannel\n",
            "tests/test_python.py::test_grayscale[detect-model0-coco8.yaml]\n",
            "tests/test_python.py::test_grayscale[pose-model1-coco8-pose.yaml]\n",
            "tests/test_python.py::test_grayscale[obb-model2-dota8.yaml]\n",
            "tests/test_python.py::test_grayscale[segment-model4-coco8-seg.yaml]\n",
            "  /usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/utils.py:1447: OnnxExporterWarning: Exporting to ONNX opset version 22 is not supported. by 'torch.onnx.export()'. The highest opset version supported is 20. To use a newer opset version, consider 'torch.onnx.export(..., dynamo=True)'. \n",
            "    warnings.warn(\n",
            "\n",
            "tests/test_exports.py::test_export_onnx[True]\n",
            "tests/test_exports.py::test_export_tflite\n",
            "tests/test_python.py::test_utils_benchmarks\n",
            "tests/test_python.py::test_multichannel\n",
            "tests/test_python.py::test_grayscale[detect-model0-coco8.yaml]\n",
            "tests/test_python.py::test_grayscale[pose-model1-coco8-pose.yaml]\n",
            "tests/test_python.py::test_grayscale[obb-model2-dota8.yaml]\n",
            "tests/test_python.py::test_grayscale[segment-model4-coco8-seg.yaml]\n",
            "  /usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/symbolic_opset9.py:5353: UserWarning: Exporting aten::index operator of advanced indexing in opset 22 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
            "    warnings.warn(\n",
            "\n",
            "tests/test_exports.py::test_export_coreml\n",
            "  /usr/local/lib/python3.12/dist-packages/coremltools/optimize/torch/palettization/fake_palettize.py:82: SyntaxWarning: invalid escape sequence '\\_'\n",
            "    n_bits (:obj:`int`): Number of palettization bits. There would be :math:`2^{n\\_bits}` unique weights in the ``LUT``.\n",
            "\n",
            "tests/test_exports.py::test_export_coreml\n",
            "  /usr/local/lib/python3.12/dist-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:273: UserWarning: Output, '1441', of the source model, has been renamed to 'var_1441' in the Core ML model.\n",
            "    warnings.warn(msg.format(var.name, new_name))\n",
            "\n",
            "tests/test_exports.py::test_export_coreml\n",
            "  /usr/local/lib/python3.12/dist-packages/coremltools/converters/mil/mil/passes/defs/optimize_repeat_ops.py:433: RuntimeWarning: overflow encountered in cast\n",
            "    max(cur_range.low, tmp_range.low), min(cur_range.high, tmp_range.high)\n",
            "\n",
            "tests/test_exports.py::test_export_tflite\n",
            "  /content/ultralytics/ultralytics/utils/export/tensorflow.py:35: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "    if self.format != \"imx\" and (self.dynamic or self.shape != shape):\n",
            "\n",
            "tests/test_exports.py::test_export_tflite\n",
            "  /content/ultralytics/ultralytics/utils/export/tensorflow.py:39: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "    grid_size = torch.tensor([grid_w, grid_h, grid_w, grid_h], device=boxes.device).reshape(1, 4, 1)\n",
            "\n",
            "tests/test_exports.py::test_export_imx\n",
            "  /usr/local/lib/python3.12/dist-packages/mct_quantizers/keras/quantize_wrapper.py:100: SyntaxWarning: invalid escape sequence '\\l'\n",
            "    attributes are integers representing the input index in the function\\layer's inputs.\n",
            "\n",
            "tests/test_exports.py::test_export_imx\n",
            "  /usr/local/lib/python3.12/dist-packages/mct_quantizers/common/metadata.py:28: SyntaxWarning: invalid escape sequence '\\i'\n",
            "    metadata (Dict): metadata dictionary. Should contain only string keys and string\\interger\\float values or\n",
            "\n",
            "tests/test_exports.py::test_export_imx\n",
            "  /usr/local/lib/python3.12/dist-packages/mct_quantizers/pytorch/quantize_wrapper.py:45: SyntaxWarning: invalid escape sequence '\\l'\n",
            "    representing the input index in the function\\layer's inputs.\n",
            "\n",
            "tests/test_exports.py::test_export_imx\n",
            "  /usr/local/lib/python3.12/dist-packages/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py:43: RuntimeWarning: divide by zero encountered in divide\n",
            "    z_score = np.abs(bins - mu) / sigma\n",
            "\n",
            "tests/test_exports.py::test_export_imx\n",
            "  /usr/local/lib/python3.12/dist-packages/model_compression_toolkit/core/common/quantization/quantization_params_generation/outlier_filter.py:43: RuntimeWarning: invalid value encountered in divide\n",
            "    z_score = np.abs(bins - mu) / sigma\n",
            "\n",
            "tests/test_exports.py::test_export_imx\n",
            "  /usr/local/lib/python3.12/dist-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
            "    threshold = torch.tensor(threshold, dtype=torch.float32).to(get_working_device())\n",
            "\n",
            "tests/test_exports.py::test_export_imx\n",
            "  /usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/jit_utils.py:303: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
            "    _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
            "\n",
            "tests/test_exports.py::test_export_imx\n",
            "  /usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/utils.py:711: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
            "    _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "\n",
            "tests/test_exports.py::test_export_imx\n",
            "  /usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/torchscript_exporter/utils.py:1181: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
            "    _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "\n",
            "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
            "============================= slowest 30 durations =============================\n",
            "255.46s call     tests/test_exports.py::test_export_imx\n",
            "54.53s call     tests/test_cli.py::test_predict[detect-model0-coco8.yaml]\n",
            "54.26s call     tests/test_cli.py::test_predict[obb-model2-dota8.yaml]\n",
            "54.16s call     tests/test_cli.py::test_predict[segment-model4-coco8-seg.yaml]\n",
            "54.04s call     tests/test_cli.py::test_predict[pose-model1-coco8-pose.yaml]\n",
            "40.61s call     tests/test_cli.py::test_rtdetr\n",
            "28.94s call     tests/test_python.py::test_all_model_yamls\n",
            "27.94s call     tests/test_cli.py::test_predict[classify-model3-imagenet10]\n",
            "26.32s call     tests/test_exports.py::test_export_tflite\n",
            "24.63s call     tests/test_python.py::test_track_stream[yolo26n-obb.pt]\n",
            "23.41s call     tests/test_python.py::test_track_stream[yolo26n-pose.pt]\n",
            "22.86s call     tests/test_python.py::test_track_stream[yolo26n-seg.pt]\n",
            "22.24s call     tests/test_python.py::test_grayscale[segment-model4-coco8-seg.yaml]\n",
            "22.20s call     tests/test_engine.py::test_segment\n",
            "22.19s call     tests/test_python.py::test_track_stream[yolo26n.pt]\n",
            "20.67s call     tests/test_solutions.py::test_similarity_search\n",
            "20.17s call     tests/test_cli.py::test_train[segment-model4-coco8-seg.yaml]\n",
            "20.03s call     tests/test_cli.py::test_val[segment-model4-coco8-seg.yaml]\n",
            "19.32s call     tests/test_python.py::test_track_stream[yolo11n-grayscale.pt]\n",
            "19.06s call     tests/test_engine.py::test_export\n",
            "18.86s call     tests/test_cli.py::test_solutions[isegment]\n",
            "18.83s call     tests/test_python.py::test_grayscale[pose-model1-coco8-pose.yaml]\n",
            "18.62s call     tests/test_cli.py::test_train[pose-model1-coco8-pose.yaml]\n",
            "17.41s call     tests/test_python.py::test_multichannel\n",
            "17.33s call     tests/test_engine.py::test_detect\n",
            "17.20s call     tests/test_exports.py::test_export_coreml\n",
            "17.01s call     tests/test_python.py::test_workflow\n",
            "16.85s call     tests/test_cli.py::test_val[detect-model0-coco8.yaml]\n",
            "16.50s call     tests/test_python.py::test_grayscale[detect-model0-coco8.yaml]\n",
            "16.27s call     tests/test_cli.py::test_train[detect-model0-coco8.yaml]\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m ultralytics/tests/test_cuda.py::\u001b[1mtest_checks\u001b[0m - AssertionError: assert False == True\n",
            "\u001b[31mFAILED\u001b[0m ultralytics/tests/test_cuda.py::\u001b[1mtest_amp\u001b[0m - RuntimeError: No CUDA GPUs are available\n",
            "\u001b[31mFAILED\u001b[0m ultralytics/tests/test_cuda.py::\u001b[1mtest_train\u001b[0m - ValueError: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass val...\n",
            "\u001b[31mFAILED\u001b[0m ultralytics/tests/test_cuda.py::\u001b[1mtest_autobatch\u001b[0m - RuntimeError: No CUDA GPUs are available\n",
            "\u001b[31mFAILED\u001b[0m ultralytics/tests/test_cuda.py::\u001b[1mtest_predict_sam\u001b[0m - ValueError: Invalid CUDA 'device=0' requested. Use 'device=cpu' or pass val...\n",
            "\u001b[31mFAILED\u001b[0m ultralytics/tests/test_exports.py::\u001b[1mtest_export_executorch\u001b[0m - AttributeError: '_OpNamespace' 'torchao' object has no attribute 'dequantiz...\n",
            "\u001b[31m===== \u001b[31m\u001b[1m6 failed\u001b[0m, \u001b[32m151 passed\u001b[0m, \u001b[33m13 skipped\u001b[0m, \u001b[33m44 warnings\u001b[0m\u001b[31m in 1672.88s (0:27:52)\u001b[0m\u001b[31m ======\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_icJkqGnXFv8",
        "outputId": "c58f71b2-2280-4f93-a83d-d87ab22e78a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics 8.4.7 üöÄ Python-3.12.12 torch-2.9.0+cu126 CPU (Intel Xeon CPU @ 2.00GHz)\n",
            "YOLO26n summary (fused): 122 layers, 2,408,932 parameters, 0 gradients, 5.4 GFLOPs\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 31.5¬±33.2 MB/s, size: 106.3 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/coco/labels/val2017... 4952 images, 48 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5000/5000 367.1it/s 13.6s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/coco/labels/val2017.cache\n",
            "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 30% ‚îÅ‚îÅ‚îÅ‚ï∏‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 94/313 2.1s/it 3:15<7:37"
          ]
        }
      ],
      "execution_count": null,
      "source": [
        "# Validate multiple models\n",
        "for x in 'nsmlx':\n",
        "  !yolo val model=yolo26{x}.pt data=coco.yaml"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ================= Á¨¨‰∏ÄÈò∂ÊÆµÔºöÂêéÁ´ØÁªÑ‰ª∂ÂáÜÂ§á =================\n",
        "\n",
        "print(\"‚è≥ Ê≠£Âú®ÂÆâË£ÖÂèØËßÜÂåñÁïåÈù¢Â∫ì Gradio...\")\n",
        "# 1. ÂÆâË£Ö Gradio (Colab ÈªòËÆ§Ê≤°Ë£ÖÔºåÊâÄ‰ª•ÂøÖÈ°ªÂÖàËøêË°åËøôË°å)\n",
        "# -q ÁöÑÊÑèÊÄùÊòØ quiet (ÂÆâÈùôÊ®°Âºè)Ôºå‰∏çÊòæÁ§∫‰∏ÄÂ§ßÂ†ÜÂÆâË£ÖÊó•Âøó\n",
        "!pip install -q gradio\n",
        "\n",
        "print(\"‚úÖ Gradio ÂÆâË£ÖÂÆåÊàêÔºÅÊ≠£Âú®Âä†ËΩΩ AI Ê®°Âûã...\")\n",
        "\n",
        "import gradio as gr\n",
        "from ultralytics import YOLO\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# 2. Âä†ËΩΩ AI Â§ßËÑë (YOLOv8 Nano Ê®°Âûã)\n",
        "# Â¶ÇÊûú‰Ω†‰πãÂâçË∑ëËøá SetupÔºåËøô‰∏™Êñá‰ª∂Â∫îËØ•Â∑≤ÁªèÂú®‰Ω†ÁöÑÁõÆÂΩï‰∏ã‰∫Ü\n",
        "# Â¶ÇÊûúÊ≤°ÊúâÔºåËøôË°å‰ª£Á†Å‰ºöËá™Âä®‰ªéÁΩë‰∏ä‰∏ãËΩΩÔºåÈùûÂ∏∏Êô∫ËÉΩ\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "print(\"‚úÖ Ê®°ÂûãÂä†ËΩΩÂÆåÊØïÔºÅÈöèÊó∂ÂæÖÂëΩ„ÄÇ\")\n",
        "\n",
        "\n",
        "# ================= Á¨¨‰∫åÈò∂ÊÆµÔºöÊ†∏ÂøÉÈÄªËæëÂÆö‰πâ =================\n",
        "\n",
        "# 3. ÂÆö‰πâ‚ÄúÂ§ÑÁêÜÂáΩÊï∞‚ÄùÔºöËøôÂ∞±ÂÉèÊòØÈ§êÂéÖÁöÑÂêéÂé®\n",
        "# ÂâçÁ´Ø(Gradio)ÊääËèú(ÂõæÁâá)‰º†ËøõÊù•ÔºåËøôÈáåË¥üË¥£Âä†Â∑•ÔºåÁÑ∂ÂêéÂÜçÁ´ØÂá∫Âéª\n",
        "def ai_detect_process(input_image):\n",
        "    \"\"\"\n",
        "    ËæìÂÖ•: Áî®Êà∑‰∏ä‰º†ÁöÑÂõæÁâá (PILÊ†ºÂºè)\n",
        "    ËæìÂá∫: ÁîªÂ•ΩÊ°ÜÁöÑÂõæÁâá (PILÊ†ºÂºè)\n",
        "    \"\"\"\n",
        "    if input_image is None:\n",
        "        return None\n",
        "\n",
        "    # A. Êé®ÁêÜÔºöËÆ© AI ÁúãÂõæ\n",
        "    # conf=0.25 ÊÑèÊÄùÊòØÁΩÆ‰ø°Â∫¶Ë∂ÖËøá 25% ÁöÑÊâçÁîªÂá∫Êù•ÔºåÈò≤Ê≠¢‰π±Áîª\n",
        "    results = model(input_image, conf=0.25)\n",
        "\n",
        "    # B. ÁªòÂõæÔºöÊääÊ£ÄÊµãÂà∞ÁöÑÊ°ÜÁîªÂú®ÂéüÂõæ‰∏ä\n",
        "    # results[0].plot() ËøîÂõûÁöÑÊòØ‰∏Ä‰∏™ BGR Ê†ºÂºèÁöÑ numpy Êï∞ÁªÑ (OpenCVÊ†áÂáÜ)\n",
        "    result_array_bgr = results[0].plot()\n",
        "\n",
        "    # C. È¢úËâ≤Ê†°Ê≠£ÔºöËøôÊòØÂ∑•Á®ãÂ∏àÂøÖÈ°ªÊ≥®ÊÑèÁöÑÁªÜËäÇÔºÅ\n",
        "    # OpenCV Áî®ÁöÑÊòØ BGR (ËìùÁªøÁ∫¢)ÔºåÁΩëÈ°µÊòæÁ§∫Áî®ÁöÑÊòØ RGB (Á∫¢ÁªøËìù)\n",
        "    # ‰∏ãÈù¢ËøôË°å‰ª£Á†ÅÊòØÊääÈ¢úËâ≤È°∫Â∫èÂèçËΩ¨‰∏Ä‰∏ãÔºåÂê¶Âàô‰∫∫ËÑ∏‰ºöÂèòÊàêËìùËâ≤ÁöÑÈòøÂá°Ëææ\n",
        "    result_array_rgb = result_array_bgr[..., ::-1]\n",
        "\n",
        "    # D. Ê†ºÂºèËΩ¨Êç¢ÔºöËΩ¨ÂõûÂõæÁâáÊ†ºÂºèÂèëÁªôÂâçÁ´Ø\n",
        "    return Image.fromarray(result_array_rgb)\n",
        "\n",
        "\n",
        "# ================= Á¨¨‰∏âÈò∂ÊÆµÔºöÂâçÁ´ØÁïåÈù¢Êê≠Âª∫ =================\n",
        "\n",
        "print(\"üé® Ê≠£Âú®ÊûÑÂª∫ÁΩëÈ°µÁïåÈù¢...\")\n",
        "\n",
        "# 4. Êê≠Âª∫ÁïåÈù¢Â∏ÉÂ±Ä\n",
        "# inputs: ÂÆö‰πâÂ∑¶ËæπÊ°ÜÊ°Ü (‰∏ä‰º†ÂõæÁâá)\n",
        "# outputs: ÂÆö‰πâÂè≥ËæπÊ°ÜÊ°Ü (ÊòæÁ§∫ÁªìÊûú)\n",
        "demo = gr.Interface(\n",
        "    fn=ai_detect_process,      # ÁªëÂÆöÂàöÊâçÂÜôÁöÑ‚ÄúÂêéÂé®‚ÄùÂáΩÊï∞\n",
        "    inputs=gr.Image(type=\"pil\", label=\"üì∑ ËØ∑Âú®ËøôÈáå‰∏ä‰º†ÁÖßÁâáÊàñÊãçÁÖß\"),\n",
        "    outputs=gr.Image(type=\"pil\", label=\"ü§ñ AI Ê£ÄÊµãÁªìÊûúÂ±ïÁ§∫\"),\n",
        "    title=\"ÊñπËåÇË∞®ÁöÑÊØï‰∏öËÆæËÆ° - Ë∑åÂÄíÊ£ÄÊµãÁ≥ªÁªüÂéüÂûã\",\n",
        "    description=\"\"\"\n",
        "    ### ‰ΩøÁî®ËØ¥ÊòéÔºö\n",
        "    1. ÁÇπÂáªÂ∑¶‰æßÂå∫Âüü‰∏ä‰º†‰∏ÄÂº†ÂåÖÂê´**‰∫∫**ÁöÑÁÖßÁâá„ÄÇ\n",
        "    2. Á≠âÂæÖÂá†ÁßíÈíü„ÄÇ\n",
        "    3. Âè≥‰æßÂ∞ÜÊòæÁ§∫ YOLOv8 Ê®°ÂûãÂÆûÊó∂Ê£ÄÊµãÁöÑÁªìÊûú„ÄÇ\n",
        "    *(Âü∫‰∫é Colab T4 GPU Âä†ÈÄüËøêË°å)*\n",
        "    \"\"\",\n",
        "    theme=gr.themes.Soft() # Âä†‰∏™ÁöÆËÇ§ÔºåËÆ©ÁïåÈù¢Â•ΩÁúãÁÇπ\n",
        ")\n",
        "\n",
        "\n",
        "# ================= Á¨¨ÂõõÈò∂ÊÆµÔºöÂêØÂä®ÊúçÂä°Âô® =================\n",
        "\n",
        "print(\"üöÄ Ê≠£Âú®ÂêØÂä®ÊúçÂä°... ËØ∑ÁïôÊÑè‰∏ãÊñπÁöÑ Public URL ÈìæÊé•ÔºÅ\")\n",
        "\n",
        "# 5. ÂèëÂ∞ÑÔºÅ\n",
        "# share=True ÊòØÂÖ≥ÈîÆÔºöÂÆÉ‰ºöÂÖçË¥πÁîüÊàê‰∏Ä‰∏™ÂÖ¨ÂºÄÁöÑ xxx.gradio.live ÈìæÊé•\n",
        "# ËøôÊ†∑‰Ω†Â∞±ÂèØ‰ª•ÊääÈìæÊé•ÂèëÁªôÊâãÊú∫ÔºåÊàñËÄÖÂèëÁªô‰Ω†ÁöÑÂêåÂ≠¶Áúã\n",
        "demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "id": "KbBS1AwWJhKh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        },
        "outputId": "0c62f19e-822d-48a3-aed4-f57f2ee55741"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Ê≠£Âú®ÂÆâË£ÖÂèØËßÜÂåñÁïåÈù¢Â∫ì Gradio...\n",
            "‚úÖ Gradio ÂÆâË£ÖÂÆåÊàêÔºÅÊ≠£Âú®Âä†ËΩΩ AI Ê®°Âûã...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.4.0/yolov8n.pt to 'yolov8n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 6.2MB 382.4MB/s 0.0s\n",
            "‚úÖ Ê®°ÂûãÂä†ËΩΩÂÆåÊØïÔºÅÈöèÊó∂ÂæÖÂëΩ„ÄÇ\n",
            "üé® Ê≠£Âú®ÊûÑÂª∫ÁΩëÈ°µÁïåÈù¢...\n",
            "üöÄ Ê≠£Âú®ÂêØÂä®ÊúçÂä°... ËØ∑ÁïôÊÑè‰∏ãÊñπÁöÑ Public URL ÈìæÊé•ÔºÅ\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://31f04b1136b9401b8b.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://31f04b1136b9401b8b.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created dataset file at: .gradio/flagged/dataset1.csv\n",
            "\n",
            "0: 416x640 1 airplane, 1 kite, 52.3ms\n",
            "Speed: 4.6ms preprocess, 52.3ms inference, 15.5ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "0: 416x640 1 airplane, 1 kite, 9.1ms\n",
            "Speed: 2.6ms preprocess, 9.1ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "111155555555555555555"
      ],
      "metadata": {
        "id": "RaiieSjzIOOz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}